## config.yaml
training:
  train_steps: 100000          # Base model training steps as per paper (12 hours on 8 P100 GPUs)
  big_train_steps: 300000      # Big model training steps for extended experiments (3.5 days on 8 P100 GPUs)
  gpu_count: 8
  optimizer:
    type: Adam
    beta1: 0.9
    beta2: 0.98
    epsilon: 1e-9
  learning_rate:
    formula: "d_model^-0.5 * min(step_num^-0.5, step_num * warmup_steps^-1.5)"
    warmup_steps: 4000
model:
  num_layers: 6               # Number of layers in both encoder and decoder for the base model
  d_model: 512                # Model dimension for embeddings and sub-layers
  d_ff: 2048                  # Inner-layer dimensionality for the position-wise feed-forward network
  num_heads: 8                # Number of attention heads (with d_k = d_v = 64)
  dropout: 0.1                # Dropout rate applied in all sub-layers and embedding sums
  label_smoothing: 0.1        # Label smoothing value used during training
  positional_encoding: "sinusoidal"  # Use sinusoidal positional encodings
data:
  tokens_per_batch: 25000     # Approximately 25000 source tokens and 25000 target tokens per batch
  tokenization:
    method: "BPE/WordPiece"   # Tokenization method as outlined in the paper
    en_de_vocab_size: 37000   # Vocabulary size for WMT 2014 English-to-German task
    en_fr_vocab_size: 32000   # Vocabulary size for WMT 2014 English-to-French task
    wsj_vocab_size:            # Vocabulary sizes for English constituency parsing (WSJ)
      wsj_only: 16000         # 16K tokens when using only WSJ data
      semi_supervised: 32000  # 32K tokens when using additional corpora
checkpoint:
  interval_minutes: 10        # Interval for checkpointing (e.g., saved every 10 minutes)
inference:
  translation:
    beam_size: 4             # Beam search size for translation tasks
    length_penalty: 0.6      # Length penalty applied during beam search for translation
    max_length_offset: 50    # Maximum output length = input length + 50 for translation
  parsing:
    beam_size: 21            # Beam search size for constituency parsing
    length_penalty: 0.3      # Length penalty applied during beam search for parsing
    max_length_offset: 300   # Maximum output length = input length + 300 for parsing